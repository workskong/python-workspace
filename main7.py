import os
import streamlit as st
import tiktoken
import time

from loguru import logger

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.chat_models import ChatOllama

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.document_loaders import Docx2txtLoader
from langchain_community.document_loaders import UnstructuredPowerPointLoader
from langchain_community.document_loaders.csv_loader import CSVLoader

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.storage import LocalFileStore
from langchain.embeddings import CacheBackedEmbeddings

from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.memory import StreamlitChatMessageHistory

from langchain_community.callbacks import get_openai_callback

from langchain.chains import ConversationChain
from langchain.prompts import PromptTemplate


if not os.path.exists(".cache"):
    os.mkdir(".cache")
if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")
if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")
if not os.path.exists(".cache/files_temp"):
    os.mkdir(".cache/files_temp")
if not os.path.exists(".cache/faiss_index"):
    os.mkdir(".cache/faiss_index")

cache_faiss = ".cache/faiss_index"
files = os.listdir(f"./.cache/files/")
files_temp = os.listdir(f"./.cache/files_temp/")

st.set_page_config(
    page_title="workskong",
    page_icon=":books:")

with open(".venv/style.css" ) as css:
    st.markdown( f'<style>{css.read()}</style>' , unsafe_allow_html= True)

st.title("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”")

col1, col2 = st.columns(2)
with col1:
    with st.expander("ì„œë¹„ìŠ¤ ì†Œê°œ"):
        st.caption(
            """
            - ì¸í„°ë„·ì„ ì‚¬ìš©í•´ì•¼ í•˜ëŠ” ìœ ë£Œ LLM APIë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ 
            ìƒì—…ì ìœ¼ë¡œë„ ë¬´ë£Œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¡œì»¬ LLMì„ ì´ìš©í•´ì„œ ë§Œë“  ë¡œì»¬ AIì…ë‹ˆë‹¤.
            - ì±„íŒ…ì„ ìœ„í•´ ì‚¬ìš©ëœ LLMì€ LLama3ì´ë©°,
            ë” ë§ì€ ê¸°ëŠ¥ì„ ì§€ì›í•˜ê¸° ìœ„í•´ì„œ í–¥ í›„ ë‹¤ì–‘í•œ LLM ëª¨ë¸ì„ í™œìš©í•  ì˜ˆì •ì´ì˜ˆìš”.
            - ì—¬ê¸°ì„œ ë¡œì»¬ AIë¼ëŠ” ë§ì€ ì´ ì„œë¹„ìŠ¤ê°€ ì œ ê°œì¸ PCì—ì„œ ì‹¤í–‰ëœë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.
            ë„ˆë¬´ ê°œì¸ì ì¸ ë¬¸ì„œë‚˜ ì €ì—ê²Œ ì•Œë ¤ì§€ê¸° ì‹«ì€ ì •ë³´ê°€ ë‹´ê¸´ ë¬¸ì„œëŠ” ì—…ë¡œë“œí•˜ì§€ ë§ˆì„¸ìš”.
            - í•œê¸€ë¡œ ë¬¼ì–´ë´ë„, ë‹µë³€ì„ ì¤ë‹ˆë‹¤. í•˜ì§€ë§Œ ê°€ê¸‰ì  ì˜ì–´ë¡œ ë¬¼ì–´ë³´ì„¸ìš”.
            - ê°œë°œê³¼ì •ì—ì„œ ì´ë¯¸ ì˜¬ë¦° ë¬¸ì„œëŠ” ì‚­ì œë  ìˆ˜ ìˆì–´ìš”.
            - ë¬¸ì„œ ì½ê¸° ê¸°ëŠ¥ì€ Retrieval Augmented Generation(RAG), ê²€ìƒ‰ì¦ê°•ìƒì„±ë¥¼ ì´ìš©í•´ì„œ ë‹µë³€í•©ë‹ˆë‹¤.
            - 500 Page ì´ìƒì˜ íŒŒì¼ì€ ê°€ê¸‰ì  ì˜¬ë¦¬ì§€ ë§ì•„ì£¼ì„¸ìš”. ì°¸ê³ ë¡œ 1,400 Page ì—…ë¡œë“œ ì‹œê°„ì€ ì•½ 5ë¶„ ë‚´ì™¸ì…ë‹ˆë‹¤.
            - ì•„ì§ êµ¬í˜„í•´ì•¼ í•  ê²Œ ë§ì´ ë‚¨ì•„ìˆì–´ìš”.
            """
            )

with col2:
    with st.expander("ëˆ„êµ¬ë¥¼ ìœ„í•œ ì„œë¹„ìŠ¤ì¸ê°€ìš”?"):
        st.caption(
            """
            - ë‚´ ëŒ€í™”ê°€ ChatGPTë‚˜ Gemini ê°™ì€ ì¸í„°ë„· ì„œë¹„ìŠ¤ë¡œ ì €ì¥ë˜ëŠ” ê²Œ ê·¸ëƒ¥ ì‹«ìœ¼ì‹  ë¶„
            - ì—¬ëŸ¬ ë¬¸ì„œë¥¼ íŒŒì¼ë¡œ ì½ì–´ì„œ ìš”ì•½ì´ í•„ìš” í•˜ì‹  ë¶„
            - íŒŒì¼ë¡œ ëœ ë¬¸ì„œë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë©”ì¼ì„ ì“°ê±°ë‚˜, ìŠ¤ì¼€ì¤„ ì‘ì„±ì´ í•„ìš”í•œ ë¶„
            - ë¬´ì œí•œìœ¼ë¡œ AIê°€ ì§€ì–´ë‚´ëŠ” ì•„ë¬´ ì˜ë¯¸ì—†ëŠ” ëŒ€í™”ê°€ í•˜ê³  ì‹¶ì€ ë¶„
            - í•  ì¼ ì—†ëŠ” ë¶„
            """
            )

tab1, tab2 = st.sidebar.tabs(["ğŸ’¾ ì €ì¥í•˜ê¸°", "ğŸ”§ ì œì–´íŒ",])

with tab2:
    st.caption("LLM ì˜µì…˜ì„ ì¡°ì • í•  ìˆ˜ ìˆì–´ìš”")
    with st.expander("ğŸŒ¶ï¸ temperature"):
        st.caption(
            """
            ë‚®ì€ ì˜¨ë„ì—ì„œëŠ” ëª¨ë¸ì´ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¨ì–´ë¥¼ 
            ì„ íƒí•˜ëŠ” ê²½í–¥ì´ ìˆì–´ ë³´ë‹¤ ì•ˆì „í•˜ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. 
            ë°˜ë©´ ë†’ì€ ì˜¨ë„ì—ì„œëŠ” ëª¨ë¸ì´ ëœ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¨ì–´ë¥¼ ì„ íƒí•  ê°€ëŠ¥ì„±ì´ ë†’ì•„ì ¸ 
            ë” ì°½ì˜ì ì´ê³  ë…ì°½ì ì¸ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            """
            )
        temperature = st.slider('ì¶œë ¥ í…ìŠ¤íŠ¸ì—ì„œ ì„ íƒí•˜ëŠ” ë‹¨ì–´ì˜ ë‹¤ì–‘ì„±ì„ ì œì–´í•©ë‹ˆë‹¤.', 0, 10, 0)

    with st.expander("ğŸ•µï¸â€â™€ï¸ top_p"):
        st.caption(
            """
            ê°’ì´ ë‚®ì„ìˆ˜ë¡ ëª¨ë¸ì€ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¨ì–´ë§Œ ê³ ë ¤í•˜ì—¬ ì•ˆì „í•˜ê³  
            ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ë°˜ë©´ ë†’ì€ ê°’ì¼ìˆ˜ë¡ ëª¨ë¸ì€ ëœ ê°€ëŠ¥ì„±ì´ 
            ë†’ì€ ë‹¨ì–´ë¥¼ ì„ íƒí•  ìˆ˜ ìˆëŠ” ì—¬ìœ ê°€ ìˆì–´ 
            ë” ì°½ì˜ì ì´ê³  ë…ì°½ì ì¸ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
            """
            )
        top_p = st.slider('ì¶œë ¥ í…ìŠ¤íŠ¸ì—ì„œ ì„ íƒí•˜ëŠ” ë‹¨ì–´ì˜ ë²”ìœ„ë¥¼ ì œì–´í•©ë‹ˆë‹¤.', 0, 10, 0)
    
    with st.expander("ğŸ¤– Length"):
        st.caption(
            """
            ì§§ì€ ê¸¸ì´ëŠ” ê°„ê²°í•˜ê³  ìš”ì•½ëœ ê²°ê³¼ë¥¼ ìƒì„±í•˜ëŠ” ë°˜ë©´, 
            ê¸´ ê¸¸ì´ëŠ” ë” ìƒì„¸í•˜ê³  í¬ê´„ì ì¸ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            """
            )
        Length = st.slider('ìƒì„±í•˜ëŠ” í…ìŠ¤íŠ¸ì˜ ê¸¸ì´ë¥¼ ì œì–´í•©ë‹ˆë‹¤.', 0, 1000, 200)

llm = ChatOllama(
    model="llama3", 
    temperature=(temperature/10), 
    top_p=(top_p/10), 
    Length=Length)

def get_text(docs,mode):
    doc_list = []
   
    for doc in docs:
        if mode == "save":
            file_name = f"./.cache/files/{doc.name}"
        else:
            file_name = f"./.cache/files_temp/{doc.name}"

        with open(file_name, "wb") as f:
            f.write(doc.getvalue())
        logger.info(file_name)
        #logger.info(f"./.cache/files/{file_name}")

        if '.pdf' in doc.name:
            loader = PyPDFLoader(file_name)
            documents = loader.load_and_split()
        elif '.docx' in doc.name:
            loader = Docx2txtLoader(file_name)
            documents = loader.load_and_split()
        elif '.pptx' in doc.name:
            loader = UnstructuredPowerPointLoader(file_name)
            documents = loader.load_and_split()
        doc_list.extend(documents)

        if mode == "temp":
            os.remove(file_name)

    return doc_list

def tiktoken_len(text):
    tokenizer = tiktoken.get_encoding("cl100k_base")
    tokens = tokenizer.encode(text)
    return len(tokens)

def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=100,
        chunk_overlap=50,
        length_function=tiktoken_len
        )
    chunks = text_splitter.split_documents(text)
    return chunks

def embeddings():
    # model_name = model_name="jhgan/ko-sroberta-multitask"
    model_name = model_name="BAAI/bge-m3"
    model_kwargs = {'device': 'cuda'}
    encode_kwargs = {'normalize_embeddings': True}
    hfe = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )
    return hfe

def get_vectorstore(text_chunks,mode):
    start_time = time.time()

    if mode == "save":
        file_dir = f"./.cache/embeddings/cache"
    else:
        file_dir = f"./.cache/embeddings/cache_temp"

    cache_dir = LocalFileStore(file_dir)
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings(), cache_dir)

    new = FAISS.from_documents(text_chunks, cached_embeddings)

    if mode == "save":
        try:
            old = FAISS.load_local(cache_faiss, 
                                embeddings(), 
                                allow_dangerous_deserialization=True)
            old.merge_from(new)
            old.save_local(cache_faiss)
            vs = old

        except RuntimeError as e:
            new.save_local(cache_faiss)
            vs = new
    else:
        vs = new

    end_time = time.time()
    elapsed_time = end_time - start_time
    st.write(f"Embedding took {elapsed_time:.2f} seconds")

    return vs

def get_conversation_chain_rag(vetorestore):
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        chain_type="stuff",
        retriever=vetorestore.as_retriever(search_type='mmr', verbose=True, top_k=1),
        memory=ConversationBufferMemory(memory_key='chat_history', 
                                        return_messages=True, 
                                        output_key='answer'),
        return_source_documents=True,
        verbose=True
    )
    return conversation_chain

def get_conversation_chain():
    template = """
    You are AI assistant
    {chat_history}
    {question}
    """

    prompt = PromptTemplate.from_template(template)

    conversation_chain = ConversationChain(
        llm=llm,
        prompt=prompt,
        memory=ConversationBufferMemory(memory_key="chat_history"),
        input_key="question",
        output_key='answer')
    return conversation_chain

with tab1:
    st.caption("ì €ì¥ëœ ë¬¸ì„œëŠ” ëŒ€í™”ì— í™œìš©ë  ìˆ˜ ìˆì–´ìš”")
    with st.expander("ë¬¸ì„œ ì˜¬ë¦¬ê¸°"):
        # File Upload
        uploaded_files = st.file_uploader(
            "ë¨¼ì € íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”",
            type=["pdf", "pptx", "docx"],
            accept_multiple_files=True
        )
        if upload_button := st.button("ğŸ‡ ì €ì¥"):  
            if uploaded_files == []:
                st.toast("ë¨¼ì € íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”")
            else:
                with st.spinner('ì ê¹ë§Œìš”...'):
                    start_time = time.time()

                    files_text = get_text(uploaded_files,"save")
                    text_chunks = get_text_chunks(files_text)
                    vs = get_vectorstore(text_chunks,"save")
                
                    end_time = time.time()
                    elapsed_time = end_time - start_time
                    st.write(f"Loading took {elapsed_time:.2f} seconds")

                    st.session_state.conversation = get_conversation_chain_rag(vs)

        st.caption(
            """
            ì €ì¥ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´, ì„œë²„ì— íŒŒì¼ê³¼ ì„ë² ë”©ê²°ê³¼ê°€
            Vector Dabaseì— ì €ì¥ë˜ë©° ë‹µë³€í•  ë•Œ í™œìš©í•©ë‹ˆë‹¤.
            """
        )

tab1, tab2, tab3 = st.tabs(["ğŸ‘„ ì•„ë¬´ë§ì´ë‚˜ í•´ë³´ì„¸ìš”", 
                            "ğŸ“˜ ë¬¸ì„œì— ìˆëŠ” ë‚´ìš©ì„ ëŒ€ì‹  ì½ì–´ë“œë ¤ìš”", 
                            "ğŸ“š ì´ë¯¸ ì½ì€ ë¬¸ì„œë¥¼ ë’¤ì ¸ë³¼ ìˆ˜ ìˆì–´ìš”"])

with tab1:
    st.write(
    """
    - ì´ë¯¸ í•™ìŠµëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì•„ë¬´ë§ì— ë‹µí•´ì¤„ê±°ì˜ˆìš”.
    """
    )
with tab2:
    # Temporary File Upload
    st.write(
    """
    - ë¬¸ì„œë¥¼ ì„ì‹œë¡œ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸í•´ë³´ì„¸ìš”.
    > ì—¬ê¸°ì„œ ì—…ë¡œë“œí•œ íŒŒì¼ì€ ì €ì¥ë˜ì§€ ì•Šê³  ì‚­ì œë©ë‹ˆë‹¤. 
    > ë°©ê¸ˆ ì—…ë¡œë“œí•œ ë¬¸ì„œë¥¼ ê·¼ê±°ë¡œ ë‹µë³€ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    )
    st.caption(
    """
    - ì„œë²„ì— ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´ ì™¼ìª½ ì‚¬ì´ë“œë°”ì— ìˆëŠ” "ë¬¸ì„œ ì˜¬ë¦¬ê¸°" í›„ "ì €ì¥" ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì„¸ìš”.
    """
    )
    uploaded_files = st.file_uploader(
        "ë¬¸ì„œ ì˜¬ë¦¬ê¸°",
        type=["pdf", "pptx", "docx"],
        accept_multiple_files=True
    )
    if uploaded_files:
        with st.spinner('ì ê¹ë§Œìš”...'):
            start_time = time.time()

            files_text = get_text(uploaded_files,"temp")
            text_chunks = get_text_chunks(files_text)
            vs = get_vectorstore(text_chunks,"temp")
        
            end_time = time.time()
            elapsed_time = end_time - start_time
            st.write(f"Loading took {elapsed_time:.2f} seconds")
            st.session_state.conversation = get_conversation_chain_rag(vs)           

with tab3:
    st.write(
    """
    - ì €ì¥ëœ ë¬¸ì„œë¥¼ ë’¤ì ¸ë³¼ê¹Œìš”?
    > ë¶ˆëŸ¬ì˜¤ê¸° ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì €ì¥ëœ Vector Tableì„ ì½ì–´ì„œ ë‹µë³€í•  ë•Œ í™œìš©í•©ë‹ˆë‹¤.
    > ì €ì¥ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜í•´ì„œ ë‹µë³€ì„ í•˜ì§€ë§Œ ê²°ê³¼ë¥¼ ë„ˆë¬´ ë¯¿ì§„ ë§ˆì„¸ìš”.
    > ë¶ˆëŸ¬ì˜¨ íŒŒì¼ ì •ë³´ëŠ” ì•„ë˜ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.
    """
    )
    st.caption(
    """
    - ì„œë²„ì— ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´ ì™¼ìª½ ì‚¬ì´ë“œë°”ì— ìˆëŠ” "ë¬¸ì„œ ì˜¬ë¦¬ê¸°" í›„ "ì €ì¥" ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì„¸ìš”.
    """
    )
    # Upload Database
    # selected_doc = st.selectbox(
    #    "Document Library",
    #    files,
    #    index=None,
    #    placeholder="All",
    #    key="selected_doc"
    #    )
    #
    #if selected_doc:
    #    st.write("You selected:", selected_doc)
    #    if 'selected_doc' not in st.session_state:
    #            st.session_state['selected_doc'] = selected_doc
    with st.expander("ğŸ“˜ ì €ì¥ëœ ë¬¸ì„œ ì—´ëŒ"):
        for file in files:
            st.caption(file)

    if load_db := st.button("ë¶ˆëŸ¬ì˜¤ê¸°"):
        start_time = time.time()

        vs = FAISS.load_local(cache_faiss, embeddings(), allow_dangerous_deserialization=True)
        
        end_time = time.time()
        elapsed_time = end_time - start_time
        st.write(f"Loading took {elapsed_time:.2f} seconds")
        st.session_state.conversation = get_conversation_chain_rag(vs)

def init_chat_input():
    if 'refLoad' not in st.session_state:
        st.session_state['refLoad'] = False
        chat_input="ğŸ’¬ ì•„ì§ ì°¸ê³ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ì•Šìœ¼ì…¨ë„¤ìš”. ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ì°¸ê³ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ê±°ë‚˜ ì„ì‹œë¡œ íŒŒì¼ì„ ì˜¬ë ¤ë³´ì„¸ìš”. ë¬¼ë¡  ì°¸ê³ ë¬¸ì„œê°€ ì—†ì–´ë„ ëŒ€í™”ê°€ ê°€ëŠ¥í•´ìš”."
    else:
        chat_input="ğŸ’¬ ì°¸ê³ ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸°ê°€ ì„±ê³µí–ˆì–´ìš”. ì´ì œ ëŒ€í™”ì—ì„œ ì°¸ê³ ë¬¸ì„œì— ìˆëŠ” ì •ë³´ë¥¼ í™œìš©í• ê±°ì˜ˆìš”."
    return chat_input

# Chat logic
if query := st.chat_input("Enter a prompt here"):

    if 'conversation' not in st.session_state:
        st.session_state.conversation = get_conversation_chain()

    if 'messages' not in st.session_state:
        st.session_state['messages'] = [{"role": "assistant",
                                        "content": "ì§ˆë¬¸ì— ë‹µë³€ì„ ë“œë¦½ë‹ˆë‹¤."}]

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    history = StreamlitChatMessageHistory(key="chat_messages")

    st.session_state.messages.append({"role": "user", "content": query})

    with st.chat_message("user"):
        st.markdown(query)

    with st.chat_message("assistant"):
        chain = st.session_state.conversation

        start_time = time.time()

        # topic = st.session_state['selected_doc']
        query = (f"{query}")

        with st.spinner("ìƒê°ì¤‘..."):
            result = chain({"question": query})
            with get_openai_callback() as cb:
                st.session_state.chat_history = result['chat_history']
            response = result['answer']
            if 'source_documents' in result:
                source_documents = result['source_documents']

                st.markdown(response)
                with st.expander("ì°¸ì¡°í•œ ë¬¸ì„œ"):
                    for i, source_doc in enumerate(source_documents):
                        st.markdown(f"Source: {source_doc.metadata['source']}", 
                                    help=source_doc.page_content)
            else:
                st.markdown(response)

            # Add assistant message to chat history
            st.session_state.messages.append({"role": "assistant", "content": response})

        end_time = time.time()
        elapsed_time = end_time - start_time 
        st.write(f"Genarating took {elapsed_time:.2f} seconds")
